#!/usr/bin/env python
import argparse
import datetime
import os

import pytorch_lightning as pl
import torch
import yaml
from IPython import embed
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
from pytorch_lightning.loggers import CSVLogger, WandbLogger

from parrot import get_directory
from parrot.brnn_architecture import BRNN_MtM, BRNN_MtO, ParrotDataModule
from parrot.tools import validate_args


def determine_matmul_precision():
    """returns True if the GPU supports Tensor Core matmul operations, False otherwise

    Returns
    -------
    bool
        A boolean indicating whether the GPU supports Tensor Core matmul operations
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        return torch.cuda.get_device_properties(device).major >= 7
    else:
        return False


def parse_args():
    """Parse command line arguments with sensible defaults.
    
    Returns
    -------
    dict
        Dictionary containing all configuration parameters with defaults applied.
    """
    parser = argparse.ArgumentParser()

    # Required arguments
    parser.add_argument(
        "--config", default=None, help="Path to the configuration file (optional)."
    )
    parser.add_argument(
        "--num_nodes",
        default=1,
        type=int,
        help="Number of nodes to run on",
    )
    parser.add_argument(
        "--num_classes",
        default=1,
        type=int,
        help="Number of classes. For regression, this value must be set to 1.",
    )
    parser.add_argument(
        "--datatype",
        default="residues",
        help="Type of data. Must be 'sequence' or 'residues'.",
    )
    parser.add_argument(
        "--tsv_file", 
        default=None,
        help="Path to the training TSV file."
    )
    parser.add_argument(
        "--split_file",
        default=None,
        help="Path to the file indicating the train/validation/test split.",
    )
    parser.add_argument(
        "--study_name", default="parrot_study", help="Name of the study. Used for WandB logging."
    )
    parser.add_argument(
        "--batch_size", default=256, type=int, help="The batch size of the model."
    )
    parser.add_argument(
        "--ignore_warnings",
        default=False,
        action="store_true",
        help="Optionally ignore parrot warnings.",
    )
    parser.add_argument(
        "--gpu_id", nargs="+", type=int, default=[0], help="GPU device ID(s) to use."
    )
    parser.add_argument(
        "--max_epochs", default=100, type=int, help="Maximum number of training epochs."
    )
    parser.add_argument(
        "--monitor", 
        default="epoch_val_loss", 
        help="Metric to monitor for early stopping and checkpointing. Available options: epoch_val_loss, epoch_val_rsquare (regression), epoch_val_accuracy, epoch_val_f1score, epoch_val_auroc, epoch_val_precision, epoch_val_mcc (classification)."
    )
    parser.add_argument(
        "--output_network", default="parrot_model.ckpt", help="Output path for the trained model."
    )
    parser.add_argument(
        "--save_splits", action="store_true", default=False, help="Save train/val/test splits to file."
    )
    parser.add_argument(
        "--encode", default="onehot", help="Encoding scheme for sequences."
    )
    parser.add_argument(
        "--set_fractions", nargs=3, type=float, default=[0.6, 0.25, 0.15], 
        help="Train, validation, test split fractions."
    )
    parser.add_argument(
        "--force_cpu", action="store_true", default=False, help="Force CPU usage even if GPU is available."
    )
    parser.add_argument(
        "--silent", action="store_true", default=False, help="Run in silent mode."
    )
    parser.add_argument(
        "--num_workers", type=int, default=None, help="Number of data loader workers."
    )
    parser.add_argument(
        "--distributed", action="store_true", default=False, help="Enable distributed training."
    )

    # Optional overrides to default configs
    parser.add_argument(
        "--optimizer_name",
        default="SGD",
        help="Optimizer to use. Supported values: 'AdamW', 'SGD'.",
    )
    parser.add_argument(
        "--learn_rate",
        type=float,
        default=0.01,
        help="The learning rate of the optimizer.",
    )
    parser.add_argument(
        "--num_lstm_layers", type=int, default=1, help="Number of LSTM layers."
    )
    parser.add_argument(
        "--lstm_hidden_size",
        type=int,
        default=20,
        help="Number of hidden units in the LSTM layers.",
    )
    parser.add_argument(
        "--num_linear_layers", type=int, default=1, help="Number of linear layers."
    )
    parser.add_argument(
        "--linear_hidden_size",
        type=int,
        default=128,
        help="Number of hidden units in the linear layers.",
    )
    parser.add_argument(
        "--dropout",
        type=float,
        default=0.25,
        help="Dropout rate on dense linear layers.",
    )

    # SGD-specific parameters
    parser.add_argument(
        "--momentum",
        type=float,
        default=0.9,
        help="Momentum value for the SGD optimizer.",
    )

    # AdamW-specific parameters
    parser.add_argument(
        "--beta1",
        type=float,
        default=0.9,
        help="Beta1 value for the Adam-based optimizer.",
    )
    parser.add_argument(
        "--beta2",
        type=float,
        default=0.999,
        help="Beta2 value for the Adam-based optimizer.",
    )
    parser.add_argument(
        "--eps",
        type=float,
        default=1e-8,
        help="Epsilon value for the Adam-based optimizer.",
    )
    parser.add_argument(
        "--weight_decay",
        type=float,
        default=1e-6,
        help="Weight decay value for the Adam-based optimizer.",
    )

    # set min epochs
    parser.add_argument(
        "--min_epochs", default=10, type=int, help="Minimum number of training epochs."
    )

    args = parser.parse_args()
    
    # Convert args to config dictionary
    config = {}
    
    # If config file is provided, load it first
    if args.config is not None:
        try:
            with open(args.config) as config_file:
                yaml_config = yaml.safe_load(config_file)
            # Handle case where YAML file is empty or None
            if yaml_config is not None:
                # Extract values from YAML structure if it uses the nested format
                for key, value in yaml_config.items():
                    if isinstance(value, dict) and "value" in value:
                        config[key] = value["value"]
                    else:
                        config[key] = value
        except FileNotFoundError:
            print(f"Warning: Config file {args.config} not found. Using command line arguments and defaults.")
        except Exception as e:
            print(f"Error loading config file: {e}")
    
    # Override with command line arguments
    for arg_name, arg_value in vars(args).items():
        if arg_name != "config":
            # If the command line argument is None and we have it from config, don't override
            if arg_value is not None or arg_name not in config:
                config[arg_name] = arg_value
    
    # Validate that required arguments are present
    if "tsv_file" not in config or config["tsv_file"] is None:
        parser.error("--tsv_file is required either as a command line argument or in the config file")
    
    return config


def main(config):
    if determine_matmul_precision():
        torch.set_float32_matmul_precision("high")

    # Validate arguments and initialize network
    data_file = validate_args.check_file_exists(config["tsv_file"], "Datafile")
    network_file = os.path.abspath(config["output_network"])
    filename_prefix, output_dir = validate_args.split_file_and_directory(network_file)

    if config["split_file"]:
        print(config["split_file"])
        split_file = validate_args.check_file_exists(
            config["split_file"], "Split-file"
        )
    else:
        split_file = None

    save_splits_output = (
        filename_prefix + "_split_file.txt" if config["save_splits"] else None
    )

    encoding_scheme, encoder, input_size = validate_args.set_encoding_scheme(
        config["encode"]
    )
    
    # Set default num_workers if not specified
    if config["num_workers"] is None:
        config["num_workers"] = (
            os.cpu_count() if os.cpu_count() <= 32 else os.cpu_count() // 4
        )
    
    # Set up the ParrotDataModule
    datamodule = ParrotDataModule(
        data_file,
        num_classes=config["num_classes"],
        datatype=config["datatype"],
        batch_size=config["batch_size"],
        encode=config["encode"],
        fractions=config["set_fractions"],
        split_file=split_file,
        excludeSeqID=config.get("excludeSeqID", None),
        ignore_warnings=config["ignore_warnings"],
        save_splits=config["save_splits"],
        num_workers=config["num_workers"],
        distributed=config["distributed"]
    )
    
    # Validate monitor parameter based on problem type
    valid_metrics = ["epoch_val_loss"]
    if datamodule.problem_type == "regression":
        valid_metrics.extend(["epoch_val_rsquare"])
    else:  # classification
        valid_metrics.extend(["epoch_val_accuracy", "epoch_val_f1score", "epoch_val_auroc", 
                             "epoch_val_precision", "epoch_val_mcc"])
    
    if config["monitor"] not in valid_metrics:
        raise ValueError(f"Invalid monitor metric '{config['monitor']}' for {datamodule.problem_type}. "
                        f"Valid options: {', '.join(valid_metrics)}")

    # Initialize the appropriate BRNN model
    model_kwargs = {
        "input_size": datamodule.input_size,
        "lstm_hidden_size": config["lstm_hidden_size"],
        "num_lstm_layers": config["num_lstm_layers"],
        "num_classes": config["num_classes"],
        "problem_type": datamodule.problem_type,
        "datatype": config["datatype"],
        "batch_size": config["batch_size"],
        "learn_rate": config["learn_rate"],
        "distributed": config["distributed"],
        "optimizer_name": config["optimizer_name"],
        "num_linear_layers": config["num_linear_layers"],
        "linear_hidden_size": config["linear_hidden_size"],
        "dropout": config["dropout"]
    }
    
    # Add optimizer-specific parameters
    if config["optimizer_name"] == "SGD":
        model_kwargs["momentum"] = config["momentum"]
    elif config["optimizer_name"] in ["AdamW", "Adam"]:
        model_kwargs["beta1"] = config["beta1"]
        model_kwargs["beta2"] = config["beta2"]
        model_kwargs["eps"] = config["eps"]
        model_kwargs["weight_decay"] = config["weight_decay"]

    if config["datatype"] == "sequence":
        model = BRNN_MtO(**model_kwargs)
    elif config["datatype"] == "residues":
        model = BRNN_MtM(**model_kwargs)
    else:
        raise ValueError(
            f"Invalid datatype {config['datatype']}. Must be 'sequence' or 'residues'."
        )

    # Determine monitoring mode based on metric
    monitor_metric = config["monitor"]
    if monitor_metric in ["epoch_val_loss"]:
        monitor_mode = "min"
    else:
        # For accuracy, R², F1, AUROC, precision, MCC - higher is better
        monitor_mode = "max"
    
    # Set up callbacks
    early_stop_callback = EarlyStopping(
        monitor=monitor_metric, min_delta=0.00, patience=3, verbose=True, mode=monitor_mode
    )

    # Create safe filename for checkpoint based on metric name
    metric_safe_name = monitor_metric.replace("epoch_", "").replace("val_", "")
    checkpoint_callback = ModelCheckpoint(
        monitor=monitor_metric,
        dirpath="checkpoints/",
        filename=f"epoch{{epoch:02d}}-{metric_safe_name}{{{monitor_metric}:.4f}}",
        save_top_k=1,
        mode=monitor_mode,
    )

    # Set up logger
    if "WANDB_API_KEY" in os.environ:
        logger = WandbLogger(project=config["study_name"], name="parrot_run")
        logger.watch(model)
    else:
        logger = CSVLogger("logs", name="parrot_model")

    silent = config["silent"]
    device = config["gpu_id"]
    print("silent is: ", silent)
    
    # Determine accelerator and device configuration
    if torch.cuda.is_available() and not config["force_cpu"]:
        accelerator = "gpu"
        devices = device
    else:
        accelerator = "cpu"
        devices = 1  # CPU accelerator expects an integer
    
    # Set up trainer
    trainer = pl.Trainer(
        gradient_clip_val=1.0,
        precision="16-mixed",
        min_epochs=config["min_epochs"],
        max_epochs=config["max_epochs"],
        accelerator=accelerator,
        num_nodes=config["num_nodes"],
        devices=devices,
        callbacks=[early_stop_callback, checkpoint_callback],
        logger=logger,
        log_every_n_steps=100,
    )

    # Train the model
    trainer.fit(model, datamodule=datamodule)

    # Test the model
    trainer.test(model, datamodule=datamodule)

    # Save the model
    trainer.save_checkpoint(config["output_network"])

    if not config["silent"]:
        print(f'\nTest Loss: {trainer.callback_metrics["test_loss"]:.4f}')

    # If using wandb, finish the run
    if isinstance(logger, WandbLogger):
        import wandb

        wandb.finish()


if __name__ == "__main__":
    config = parse_args()
    main(config)
