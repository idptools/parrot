#!/usr/bin/python
import argparse
import datetime
import os

import pytorch_lightning as pl
import torch
import yaml
from IPython import embed
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
from pytorch_lightning.loggers import CSVLogger, WandbLogger

from parrot import get_directory
from parrot.brnn_architecture import BRNN_MtM, BRNN_MtO, ParrotDataModule
from parrot.tools import validate_args


def determine_matmul_precision():
    """returns True if the GPU supports Tensor Core matmul operations, False otherwise

    Returns
    -------
    bool
        A boolean indicating whether the GPU supports Tensor Core matmul operations
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        return torch.cuda.get_device_properties(device).major >= 7
    else:
        return False


def parse_and_write_args_to_yaml():
    """Provide optional CLI overwrites to default YAML configuration file.
    The default YAML configuration file is located in the parrot package directory under data/.
    Hyperparameter sweep search spaces are output to a YAML file in the current working directory.

    Returns
    -------
    argparse.Namespace
        argparse object containing the parsed arguments.
    """
    parser = argparse.ArgumentParser()

    # Required arguments
    parser.add_argument(
        "--config", default=None, help="Path to the configuration file."
    )
    parser.add_argument(
        "--num_classes",
        default=None,
        type=int,
        help="Number of classes. For regression, this value must be set to 1.",
    )
    parser.add_argument(
        "--datatype",
        default=None,
        help="Type of data. Must be 'sequence' or 'residues'.",
    )
    parser.add_argument(
        "--tsv_file", default=None, help="Path to the training TSV file."
    )
    parser.add_argument(
        "--split_file",
        default=None,
        help="Path to the file indicating the train/validation/test split.",
    )
    parser.add_argument(
        "--study_name", default=None, help="Name of the study. Used for WandB logging."
    )
    parser.add_argument(
        "--batch_size", default=None, type=int, help="The batch size of the model."
    )
    parser.add_argument(
        "--ignore_warnings",
        default=None,
        type=bool,
        help="Optionally ignore parrot warnings.",
    )
    parser.add_argument(
        "--gpu_id", nargs="+", type=int, help="GPU device ID(s) to use."
    )

    # Optional overrides to default configs
    parser.add_argument(
        "--optimizer_name",
        default=None,
        help="Optimizer to use. Supported values: 'AdamW', 'SGD'.",
    )
    parser.add_argument(
        "--learn_rate",
        type=float,
        default=None,
        help="The learning rate of the optimizer.",
    )
    parser.add_argument(
        "--num_lstm_layers", type=int, default=None, help="Number of LSTM layers."
    )
    parser.add_argument(
        "--lstm_hidden_size",
        type=int,
        default=None,
        help="Number of hidden units in the LSTM layers.",
    )
    parser.add_argument(
        "--num_linear_layers", type=int, default=None, help="Number of linear layers."
    )
    parser.add_argument(
        "--linear_hidden_size",
        type=int,
        default=None,
        help="Number of hidden units in the linear layers.",
    )
    parser.add_argument(
        "--dropout",
        type=float,
        default=None,
        help="Dropout rate on dense linear layers.",
    )

    # SGD-specific parameters
    parser.add_argument(
        "--momentum",
        type=float,
        default=None,
        help="Momentum value for the SGD optimizer.",
    )

    # AdamW-specific parameters
    parser.add_argument(
        "--beta1",
        type=float,
        default=None,
        help="Beta1 value for the Adam-based optimizer.",
    )
    parser.add_argument(
        "--beta2",
        type=float,
        default=None,
        help="Beta2 value for the Adam-based optimizer.",
    )
    parser.add_argument(
        "--eps",
        type=float,
        default=None,
        help="Epsilon value for the Adam-based optimizer.",
    )
    parser.add_argument(
        "--weight_decay",
        type=float,
        default=None,
        help="Weight decay value for the Adam-based optimizer.",
    )

    args = parser.parse_args()

    # Load the default config file or use the provided one
    if args.config is None:
        default_config_path = os.path.join(get_directory(), "config.yaml")
        args.config = default_config_path

    with open(args.config) as config_file:
        config = yaml.safe_load(config_file)

    # Update the configuration with CLI arguments
    for arg_name, arg_value in vars(args).items():
        if arg_name == "config" or arg_value is None:
            continue
        config[arg_name] = arg_value

    # Generate the output YAML file
    output_filename = f'{config["study_name"]["value"]}_param_sweep_{datetime.date.today().strftime("%Y_%m_%d")}.yaml'
    with open(output_filename, "w") as config_file:
        yaml.safe_dump(config, config_file)

    # Set the new config path for the return value
    args.config = output_filename
    return args


def main(config):
    if determine_matmul_precision():
        torch.set_float32_matmul_precision("high")

    # Validate arguments and initialize network
    data_file = validate_args.check_file_exists(config["tsv_file"]["value"], "Datafile")
    network_file = os.path.abspath(config["output_network"]["value"])
    filename_prefix, output_dir = validate_args.split_file_and_directory(network_file)

    if config["split_file"]["value"]:
        print(config["split_file"]["value"])
        split_file = validate_args.check_file_exists(
            config["split_file"]["value"], "Split-file"
        )
    else:
        split_file = None

    save_splits_output = (
        filename_prefix + "_split_file.txt" if config["save_splits"]["value"] else None
    )

    encoding_scheme, encoder, input_size = validate_args.set_encoding_scheme(
        config["encode"]["value"]
    )
    # Set up the ParrotDataModule
    datamodule = ParrotDataModule(
        data_file,
        num_classes=config["num_classes"]["value"],
        datatype=config["datatype"]["value"],
        batch_size=config["batch_size"]["value"],
        encode=config["encode"]["value"],
        fractions=config["set_fractions"]["value"],
        split_file=split_file,
        excludeSeqID=config.get("excludeSeqID", None),
        ignore_warnings=config["ignore_warnings"]["value"],
        save_splits=config["save_splits"]["value"],
        num_workers=config["num_workers"]["value"],
        distributed=config["distributed"]["value"]
    )

    # # Prepare the data
    # datamodule.prepare_data()
    # datamodule.setup()

    # Initialize the appropriate BRNN model
    if config["datatype"]["value"] == "sequence":
        model = BRNN_MtO(
            input_size=datamodule.input_size,
            lstm_hidden_size=config["lstm_hidden_size"]["value"],
            num_lstm_layers=config["num_lstm_layers"]["value"],
            num_classes=config["num_classes"]["value"],
            problem_type=datamodule.problem_type,
            datatype=config["datatype"]["value"],
            batch_size=config["batch_size"]["value"],
            learn_rate=config["learn_rate"]["value"],
        )
    elif config["datatype"]["value"] == "residues":
        model = BRNN_MtM(
            input_size=datamodule.input_size,
            lstm_hidden_size=config["lstm_hidden_size"]["value"],
            num_lstm_layers=config["num_lstm_layers"]["value"],
            num_classes=config["num_classes"]["value"],
            problem_type=datamodule.problem_type,
            datatype=config["datatype"]["value"],
            batch_size=config["batch_size"]["value"],
            learn_rate=config["learn_rate"]["value"],
        )
    else:
        raise ValueError(
            f"Invalid datatype {config['datatype']['value']}. Must be 'sequence' or 'residues'."
        )

    # Set up callbacks
    early_stop_callback = EarlyStopping(
        monitor="epoch_val_loss", min_delta=0.00, patience=3, verbose=True, mode="min"
    )

    checkpoint_callback = ModelCheckpoint(
        monitor="epoch_val_loss",
        dirpath="checkpoints/",
        filename="epoch{epoch:02d}-val_loss{epoch_val_loss:.2f}",
        save_top_k=1,
        mode="min",
    )

    # Set up logger
    if "WANDB_API_KEY" in os.environ:
        logger = WandbLogger(project=config["study_name"]["value"], name="parrot_run")
        logger.watch(model)
    else:
        logger = CSVLogger("logs", name="parrot_model")

    silent = config["silent"]["value"]
    device = config["gpu_id"]["value"]
    print("silent is: ", silent)
    # Set up trainer
    trainer = pl.Trainer(
        gradient_clip_val=1.0,
        precision="16-mixed",
        max_epochs=config["max_epochs"]["value"],
        accelerator="gpu"
        if torch.cuda.is_available() and not config["force_cpu"]["value"]
        else "cpu",
        devices=device,
        callbacks=[early_stop_callback, checkpoint_callback],
        logger=logger,
        log_every_n_steps=100
    )

    # Train the model
    trainer.fit(model, datamodule=datamodule)

    # Test the model
    trainer.test(model, datamodule=datamodule)

    # Save the model
    trainer.save_checkpoint(config["output_network"]["value"])

    if not config["silent"]["value"]:
        print(f'\nTest Loss: {trainer.callback_metrics["test_loss"]:.4f}')

    # If using wandb, finish the run
    if isinstance(logger, WandbLogger):
        import wandb

        wandb.finish()


if __name__ == "__main__":
    args = parse_and_write_args_to_yaml()
    with open(args.config) as config_file:
        final_config = yaml.safe_load(config_file)

    main(final_config)
