# config.yaml

###### parameters that MUST be set!! ######
tsv_file: 
    name: "tsv_file"
    value: None

num_classes:
    name: "num_classes"
    value: None

datatype:
    name: "datatype"
    value: "residues"

# this is what will be used during training and optimization for early stopping / other callbacks
monitor:
    name: "monitor"
    value: "epoch_val_loss"

min_epochs:
    name: "min_epochs"
    value: 20

max_epochs:
    name: "max_epochs"
    value: 100

# options: minimize, maximize - montior will be directed accordingly.
direction:
    name: "direction"
    value: "minimize"

min_delta:
    name: "min_delta"
    value: 0.000

# number of random trials to perform in optimization before "bayesian optimization" in hyperparameter space
warm_up_trials:
    name: "warm_up_trials"
    value: 10

# null gets read in as None aka <class 'NoneType'>                                                                                
gradient_clip_val:
    name: "gradient_clip_val"
    value: null

# this should always be as large as you can fit into memory.
# advice by google: adjust hyperparameters around batch size not adjust batch size
batch_size:
    name: "batch_size"
    value: 256

split_file:
    name: "split_file"
    value: None

study_name:
    name: "study_name"
    value: None

gpu_id:
    name: "gpu_id"
    value: [0]

###### hyperparameters to optionally adjust. ######
###### These can be overwritten from CLI but here we offer some sane defaults. ######
optimizer_name:
    name: "optimizer_name"
    choices:
        SGD: "SGD"
        
learn_rate:
    name: "learn_rate"
    min: 0.01
    max: 0.1
    log: True
                                                                         
swa_lr:
    name: "swa_lr"
    min: 0.01
    max: 0.05

num_lstm_layers:
    name: num_lstm_layers
    min: 1
    max: 2

lstm_hidden_size:
    name: lstm_hidden_size
    min: 20
    max: 45

num_linear_layers:
    name: num_linear_layers
    min: 1
    max: 3

linear_hidden_size:
    name: linear_hidden_size
    min: 128
    max: 384

dropout:
    name: dropout
    min: 0.0
    max: 0.5

# SGD parameters:
momentum:
    name: "momentum"
    min: 0.9
    max: 0.999

# these params are only used if AdamW is used as optimizer.
beta1:
    name: "beta1"
    min: 0.8
    max: 0.99

beta2:
    name: "beta2"
    min: 0.9
    max: 0.999

eps:
    name: "eps"
    min: 0.00000001
    max: 0.1
    log: True

weight_decay:
    name: "weight_decay"
    min: 0.000001
    max: 0.01
    log: True

n_trials:
    name: "n_trials"
    value: 100

ignore_warnings:
    name: "ignore_warnings"
    value: False